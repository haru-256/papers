\documentclass[uplatex, dvipdfmx]{jsarticle}

    \usepackage{amsmath, amssymb, amsthm}
    \usepackage{bm}
    \usepackage{ascmac}
    \usepackage[hiresbb]{graphicx}
    \usepackage{algpseudocode, algorithm}
    
    \theoremstyle{definition}
    \newtheorem{theorem}{定理}
    \newtheorem*{theorem*}{定理}
    \newtheorem{definition}[theorem]{定義}
    \newtheorem*{definition*}{定義}
    %%%%% argmax %%%%%
    \makeatletter
    \def\argmax{\mathop{\operator@font argmax}\limits}
    \makeatother

    %%%%% argmin %%%%%
    \makeatletter
    \def\argmin{\mathop{\operator@font argmin}\limits}
    
    % プリアンブル
    \title{Wassernstein GAN}
    \author{Martin Arjovsky et al.}
    \date{2017/1}
    \begin{document}
    \maketitle
    \abovedisplayskip=10.0pt% plus 4.0pt minus 6.0pt 
    \belowdisplayskip=10.0pt% plus 4.0pt minus 6.0pt % 
    % \setlength{\abovedisplayskip}{10pt} % 上部のマージン
    % \setlength{\belowdisplayskip}{5pt} % 下部のマージン      

    \section{Introduction}
    この論文が関係している問題は教師なし学習のそれである．主に問題は，確率分布を学習するということは
    どういうことなのか？である．古典的な回答は，確率密度を学ぶ事ということである．これはよく，パラメトリックな分布族$(P_{\theta})$を
    定義し，そしてデータにおいて尤度を最大にした$\theta$を見つけることによってなされる.

    もし，データ$\{x^{(i)}\}_{i=1}^{m}$ がある場合，以下の式を解くことでパラメータ$\theta$を求める．
    \begin{equation}
        \argmax_{\theta \in \mathbb{R}^{d}} \frac{1}{m} \sum_{i=1}^{m} \log P_{\theta}(x^{(i)})        
    \end{equation}
    もし，実データ分布$\mathbb{P}_{r}$が密度分布（連続）であるとすることができ，パラメータ化された確率密度$P_{\theta}$を$\mathbb{P}_{\theta}$
    とすると，漸近的に上式はKLダイバージェンス$KL(\mathbb{P}_{r} | \mathbb{P}_{r})$の最小化となる．
    
    存在しないであろう密度分布$\mathbb{P}_r$を推定するのではなく，私たちは固定された分布$p(z)$に従うランダムな変数$Z$を定義し，
    直接ある分布$\mathbb{P}_{\theta}$に従うサンプルを生成するパラメトリックな関数 $g_{\theta}: Z  \longmapsto \chi$（主に何らかのニューラルネットワー）
    に通すことができる．
    $\theta$を様々に変化させることによって，$\mathbb{P}_{\theta}$を変化させデータの分布$\mathbb{P}_{r}$と近づけることができる．これは次の２つにおいて役に立つ．
    まず最初に，densitiesと違い，この方法は低次元多様体に制限された分布を表すことができる．次に簡単にデータを生成できる能力は分布の数値的な値を知ることよりも
    役に立つ\footnote{例えば，超解像やセマンティックセグメンテーションの領域において入力が与えられて出力の条件付き分布を考える時に役に立つ}．
    一般には，任意の高次元密度分布と仮定し，サンプルを生成することは難しい\cite{Radford}．

    GANやVAEはこのアプローチのよく知られている例である．GANは目的関数の定義において柔軟であるが学習が難しいと知られている．

    この論文では私たちはモデルの分布$\mathbb{P}_{\theta}$とデータの分布$\mathbb{P}_{r}$がどの程度近いかを測る様々な方法や，
    同様に距離またはダイバージェンス$\rho (\mathbb{P}_{\theta}, \mathbb{P}_{r})$を定義する様々な方法に注意を向ける．
    
    この論文の主な貢献は以下の３つである．
    \begin{enumerate}
        \item 第二章では，Earth Mover (EM)距離がlearning distributionで使われる
        ポピュラーな距離，ダイバージェンスと比べて，どのように振る舞うのかの理論的な解析を行う．
        \item 第3章では，Wasserstein-GANと呼ばれ，妥当であり，効率の良いEM distance の近似を最小化するGANの
        形式を定義する．そして，対応する最適化問題が妥当，安定していることを理論的に証明する．
        \item 第4章では，経験的にWGANがGANの主な学習問題を解決することを示す．具体的には，WGANの学習はDiscriminatorとGenerator
        の学習において慎重にバランスを取ることを必要とせず，ニューラルネットワークの構造の慎重な設計も必要としない．
        GANに特有のmode dropping(collapse) 現象もまた劇的に減少させる．もっとも注目を惹きつけるWGANの実用的な利点の一つは
        Discriminatorを最適化することによって，連続的に(絶え間なく)EM距離を推定することができる能力である．
        これらの学習曲線をプロットすることはデバックやハイパーパラメータの探索に役に立つだけでなく，学習曲線は観測されたサンプルの質
        と強い相関がある．
    \end{enumerate}

    \section{Standard GANの問題点}

    \section{WGAN の改善点，利点}

    \section{実装}
    \begin{itembox}[l]{疑問点}
        \begin{enumerate}
            \item Lipschitz 関数であるために重み$w \in \mathcal{W}$に対して$[-0.01, 0.01]$等にweight clipping を行うが，この場合
            Batch Normalizationの$\beta, \gamma$はどうすれば良いのか? こちらもまたweight clippingするべき?
            なにやら，Clitic（Discriminator）にweight clippingを施すと学習が崩壊することもあるらしい．
        \end{enumerate}
        

    \end{itembox}
    % 参考文
    \newpage
    \begin{thebibliography}{10}
        \bibitem{Radford}
        Radford M. Neal. Annealed importance sampling. 
        Statistics and Computing, 11(2):125–139, April 2001.
    \end{thebibliography}
    
    \end{document}