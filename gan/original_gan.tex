\documentclass[uplatex]{jsarticle}

\usepackage{amsmath, amssymb, amsthm}
\usepackage{bm}
    \begin{document}

    \section{GAN}
    \subsection{GANのLossについて}
    DiscriminatorのLossは以下の通り．
    \begin{align}
        &\mathbb{E}_{\bm{x}\sim p_{data}(\bm{x})}\left[\log D(\bm{x})\right] + \mathbb{E}_{\bm{z} \sim {p_{z}(\bm{z})}}\left[\log{\left(1-D\left(G \left(\bm{z}\right) \right) \right)}\right]  \nonumber \\ 
        &=\mathbb{E}_{\bm{x}\sim p_{data}(\bm{x})}\left[\log D(\bm{x})\right] + \mathbb{E}_{\bm{x} \sim {p_{g}(\bm{x})}}\left[\log{\left(1-D(\bm{x}) \right)}\right] \label{eq:d_loss}
    \end{align}
    ただし，$\bm{x} \sim p_{g}(\bm{x})$ を $\bm{x} = D\left( G(\bm{z})\right), \bm{z} \sim p_{g}(\bm{x})$ とした．
    また，GeneratorのLossは以下の通り．
    \begin{align}
        \mathbb{E}_{\bm{x} \sim {p_{g}(\bm{x})}}\left[\log{\left(1-D(\bm{x}) \right)}\right] 
    \label{eq:g_loss}
    \end{align}
    ここで，$p_{data}(\bm{x}), p_{g}(\bm{x}), p_{z}(\bm{z}) $はそれぞれデータの生成分布と生成モデルからの出力分布，ノイズの分布を表す．

    ただし，式 (\ref{eq:d_loss}), 式 (\ref{eq:g_loss})はデータの生成分布，生成モデルからの出力分布で期待値を取っているが
    ，実際は以下の経験分布(瀧~\cite{瀧} p94 式 (5.2) ~ 参照)を用いて近似的に汎化誤差を見積もった Lossを目的関数として最適化を行う．
    \begin{align}
        &\frac{1}{m} \sum_{i=1}^{m} \left[ \log D(\bm{x}_i)  +  \log{\left(1-D(\bm{x}_i) \right)} \right] \\
        &\frac{1}{m} \sum_{i=1}^{m} \log \left( 1 - D \left( G(\bm{z}_i) \right) \right)
    \end{align}
    $m$はデータ数を表す．

    \section{Loss の数学的解析}
    GANは以下の価値関数で表される，DiscriminatorとGeneratoのtwo-player minmax game を行う．
    \begin{equation}
        \min_G \max_D V(D, G) = \mathbb{E}_{\bm{x}\sim p_{data}(\bm{x})}\left[\log D(\bm{x})\right] + \mathbb{E}_{\bm{z} \sim {p_{z}(\bm{z})}}\left[\log{\left(1-D\left(G \left(\bm{z}\right) \right) \right)}\right]
    \end{equation}
    この時，Generatorが固定のもと，$\max_D V(D, G)$ の最適値は
    \[ D^{\ast}_{G} (\bm{x})  = \frac{p_{data}(\bm{x})}{p_{data}(\bm{x}) + p_{g}(\bm{x})} \]
    である．
    \begin{proof}
        GANの評価関数より，
        \begin{align}
            V (D, G) =& \mathbb{E}_{\bm{x}\sim p_{data}(\bm{x})}\left[\log D(\bm{x})\right] + \mathbb{E}_{\bm{z} \sim {p_{z}(\bm{z})}}\left[\log{\left(1-D\left(G \left(\bm{z}\right) \right) \right)}\right] \\
                =& \int_{\bm{x}} p_{data} (\bm{x}) \log \left( D(\bm{x}) \right) d \bm{x}
                + \int_{\bm{z}} p_{\bm{z}} (\bm{z}) \log \left( 1-D \left( G(\bm{z}) \right) \right) d\bm{z} \label{eq:expected_value_function}
        \end{align}
        ここで，$\bm{x} = G(\bm{z})$では$\bm{x} \sim p_g(\bm{x})$なので
        \begin{align}
            V (D, G)  = \int_{\bm{x}} \left\{ p_{data} (\bm{x}) \log D(\bm{x})
                        +  p_{g} (\bm{x}) \log \left( 1-D \left( \bm{x} \right) \right) \right\} d\bm{x} \label{eq:value_function}
        \end{align}
        さらに式(\ref{eq:value_function})においてDiscriminatorの最適値とその時のDiscriminatorの値を考える．
        \begin{align}
            \max_D V(D, G) = \max_D \int_{\bm{x}} \left\{ p_{data} (\bm{x}) \log D(\bm{x})
            +  p_{g} (\bm{x}) \log \left( 1-D \left( \bm{x} \right) \right) \right\} d\bm{x} \label{eq:optimal_d}
        \end{align}
        今，積分範囲は$\bm{x}$が取りうるすべての値についてなので，積分の中を最大にする$D$が最適値をとる時のDiscriminatorの値である．
        よって
        \begin{align}
            &\max_D \; p_{data} (\bm{x}) \log D(\bm{x})
            +  p_{g} (\bm{x}) \log \left( 1-D \left( \bm{x} \right) \right) 
        \end{align}
        を考える．また，関数$a\log x + b \log(1-x)$は
        \begin{equation}
            x = \frac{a}{a+b}
        \end{equation}
        で最大値をとる．よって，式 (\ref{eq:optimal_d}) においてDiscriminatorの最適値を与える時のとる値$D^{\ast}_G (\bm{x})$は
        \begin{equation}
            D^{\ast}_G (\bm{x}) = 
            \frac{p_{data} (\bm{x})}{p_{data} (\bm{x}) + p_g (\bm{x})} \label{eq:optimal}
        \end{equation}
        である．またこの時の式 (\ref{eq:expected_value_function})の値 (最適値)$C(G)$は
        \begin{align}
            C(G) = &\max_D V (D, G) \\
            &= V (D^{\ast}_G , G) \\
            &=\mathbb{E}_{\bm{x}\sim p_{data}(\bm{x})}\left[\log D^{\ast}_G(\bm{x})\right] 
            + \mathbb{E}_{\bm{z} \sim {p_{z}(\bm{z})}}\left[\log{\left(1-D^{\ast}_G\left(G \left(\bm{z}\right) \right) \right)}\right] \\
            &= \mathbb{E}_{\bm{x}\sim p_{data}(\bm{x})}\left[\log D^{\ast}_G(\bm{x})\right] 
            + \mathbb{E}_{\bm{x} \sim {p_{g}(\bm{x})}}\left[\log{\left(1-D^{\ast}_G\left(\bm{x}  \right) \right)}\right] \\
            &= \mathbb{E}_{\bm{x}\sim p_{data}(\bm{x})}\left[\log \frac{p_{data} (\bm{x})}{p_{data} (\bm{x}) + p_g (\bm{x})} \right] 
            + \mathbb{E}_{\bm{x} \sim {p_{g}(\bm{x})}}\left[\log{\left(1- \frac{p_{data} (\bm{x})}{p_{data} (\bm{x}) + p_g (\bm{x})}  \right)}\right] \\
            &= \mathbb{E}_{\bm{x}\sim p_{data}(\bm{x})}\left[\log \frac{p_{data} (\bm{x})}{p_{data} (\bm{x}) + p_g (\bm{x})} \right] 
            + \mathbb{E}_{\bm{x} \sim {p_{g}(\bm{x})}}\left[\log{\frac{p_{g} (\bm{x})}{p_{data} (\bm{x}) + p_g (\bm{x})} }\right]
        \end{align}
    \end{proof}

    % 参考文献
    \newpage
    \begin{thebibliography}{10}
        \bibitem{瀧}
            瀧 雅人 「深層学習」
            講談社
        \bibitem{goodfellow}
            Ian Goodfellow, Generative Adversarial Networks \\
            https://arxiv.org/abs/1406.2661
    \end{thebibliography}

    \end{document}