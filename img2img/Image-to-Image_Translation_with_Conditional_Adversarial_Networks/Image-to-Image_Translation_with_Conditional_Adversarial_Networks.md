# Image-to-Image Translation with Conditional Adversarial Networks

# Info

- Data : 11/ 2017
- Authors : PHILLIP ISOLA,  JUN-YAN ZHU, et al.
- Journal reference: CVPR 2017

# どんなもの？

あらゆるimage から image へ変換する問題を，同一のアーキテクチャ，ロス関数で解決する手法を提案．



# 先行研究と比べてどこがすごい？

image から image へ変換する問題設定（エッジマップからの色付け，label map からの写真への変換など）は全て共通していたが，今までの手法はこれらの変換を個別の問題として考えていた．本論文ではimage から image へと変換する問題をimage-to-image translationとして定義し，どんな種類の画像変換タスクにおいても同じアーキテクチャ，アルゴリズムで対応可能な手法を提案した．



# 技術の手法のキモはどこ？

- GeneratorにはU-Net を Discriminator には PatchGAN というアーキテクチャを使用した．
  <span style="color: red; ">詳細はウェブの別資料に記載したと言っているがどこ？</span> -> おそらく付録に書いてある．

## Objective

- 先行研究の事実

  - GANの目的関数と，L2やL1 などの昔から知られていたLoss を組み合わせるのは効果的
  - cGANs では入力 $x$ に対する出力 $y$ に多様性を持たせるために Gaussian noize $z$ を入力 $x$ に加えていたが，今回の初期実験では Generator は単に noize を無視して決定的な出力を学んでしまっていた．

- 今回の実験の工夫

  - GANのロスは以下のようにした．L1の方がL2よりも画像のボヤけに強いため L1を用いている．
  $$
  \begin{align}
  Loss &= \mathcal{L}_{cGAN}(G, D) + \lambda \mathcal{L}_{L1}(G) \tag{1}\\
  \text{ここで，}&\\
  \mathcal{L}_{cGAN}(G, D) &= \mathbb{E}_{x, y} [\log D(x, y)] + \mathbb{E}_{x, z} [\log(1-D(x, G(x, z))]\\
  \mathcal{L}_{L1}(G) &= \mathbb{E}_{x, y, z} [\| y - G(x, z) \|_1]
  \end{align}
  $$
  - noize を入れる代わりに  Generatorのいくつかの層に 学習・推論ともに Dropout を適用した．



<img src="figures/fig2.png" width=100% align="middle">



## Model Architecture

### Generator
__仮説__
img2imgの問題では入力と出力では多くの低レベルな情報が共有される．例えば colorization では入力と出力では特徴的なエッジの位置を共有する．

__工夫__
このことを考慮して論文ではU-Net 構造に skip connection を各層 $i$ と $n-i$ の間につけた．skip connection は単に $i$ 層の出力と $n-i$ 層の入力とを全channel 毎に concatenate するものである．

### Discriminator

__仮説__
以下のFig 4. の様に L1 や L2 ロスでもある程度の（低レベルな部分を捉えた）画像ができる．そこで GAN のDiscriminator には高レベルな特徴を捉えるため，局所的な画像にのみ注意を向ければ良い．

__工夫__
論文ではPatchGAN と呼ばれる方法を提案．この手法はDiscriminator に画像内の NxN のpatch について本物かどうかを判定させる．そして，このDiscmriminator を畳み込み的に画像全体に適用し，各 patchに対する出力を平均化したものを，最終的なDiscriminatorの出力とする．

__検証結果__
$N$ は画像の大きさよりもずっと小さくても良い画像を生成することが検証でわかった．より小さい PatchGAN はパラメータが少なく，より早く，任意の大きさの画像に適用することが可能である．

<img src="figures/fig4.png" width=100% align="middle">



## Optimization

### Loss

- Generatorの最適化に関しては，標準的なGAN（[Goodfellow](https://arxiv.org/abs/1406.2661)）に従い， $\log (1-D(x, G(x, z)))$ の最小化ではなく $\log(D(x, G(x, z))$ の最大化（ つまり，$- \log(D(x, G(x, z))$ の最小化）を行った．
  $$
  \underset{G}{\operatorname{argmin}} \ - \mathbb{E}_{x, z} [\log(D(x, G(x, z))]  + \mathbb{E}_{x, y, z} [\| y - G(x, z) \|_1]
  $$

- 加えて，Discriminator が Generator に対して学習が遅くなる様に，Discriminator を最適化中はロスを$1/2$ 倍した．[コード](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/pix2pix_model.py#L81-L82) によれば，Discriminator の最適化は以下の通り
  $$
  \underset{D}{\operatorname{argmax}} \ \mathbb{E}_{x, y} [\log D(x, y)] + \mathbb{E}_{x, z} [\log(1-D(x, G(x, z))]
  $$
  
## Optimizer

- 本論文では minibatch SGD を使い，Optimizer にはAdam を使用した．ハイパーパラメータとして，
  learning rate: $0.0002$
  momentum parameters: $\beta_1 = 0.5, \beta_2 = 0.999$ 

## Inference

- 推論時においても Generator はDropout を適用し，test batch での統計量を用いてBatch Norm を行う（学習時に計算された指数移動平均を用いない）．つまり，Generator は学習時と同じ振る舞いで推論を行う．

- このbatch normalization についての方法は batch size を 1と設定すると [instance normalization](https://arxiv.org/abs/1607.08022) となり，先行研究より画像生成タスクにおいて効果的ということが分かっている．Instance Norm は以下の図を参照

  <img src="https://blog.albert2005.co.jp/wp-content/uploads/2018/09/groupnorm-gn-zu.png" width=100% align="middle">

- 本論文での実験ではbatch size  を実験に応じて <span style="color: red;"> 1 ~ 10 </span>の間で設定している．



# どうやって有効だと検証した？

photo generation の様な graphics task  や semantic segmentation の様な vision task を含む様々なデータで検証した．

具体的なデータセットの一部は[ここ](https://drive.google.com/drive/u/1/folders/0B8OrLAOIArf4S3FkYzJMUS1JWmM) にある．

- 小さなデータセットの時でさえ，まずまずな結果を得た．facade データでの実験では学習時間が２時間未満であり，全てのモデルにおいて推論にかかった時間は１秒未満であった．（使用GPU: Pascal Titan X １枚）

## 評価方法
評価方法は以下の２つ
1. AMT(Amazon Mechanical Turk) を用いた人による評価
2. すでにある認識モデル（ResNet など）が物体を認識することができるほど十分に本物らしいかどうかを，測定することによる評価 -> Inception score と似ている．

__AMT__
以下参照
> Turkers were presented with a series of trials that pitted a “real” image against a “fake” image generated by our algorithm. On each trial, each image appeared for 1 second, after which the images disappeared and Turkers were given unlimited time to respond as to which was fake. The first 10 images of each session were practice and Turkers were given feedback. No feedback was provided on the 40 trials of the main experiment. Each session tested just one algorithm at a time, and Turkers were not allowed to complete more than one session. ∼50 Turkers evaluated each algorithm. All images were presented at 256 × 256 resolution. 

__FCN-score__

Inception score のようなものをsemantic segmentation にも応用したもの．
モデルには [FCN-8s](https://arxiv.org/abs/1605.06211) を用い，cityscape データセットで訓練させた．そして生成画像について（生成画像の入力となった）ラベルに対しての識別率を測った．測り方は<span style="color: red; ">おそらく</span>以下の２つ．ここで$n_{i, j}$ は最大 $k$ クラスある識別問題において，クラス $i$に属する画素がラベル $j$ と判定された画素数を意味する．

1. Pixel wise accuracy
$$
\begin{align}
\text{Accuracy} &= \frac{1}{N} \sum_{i=0}^{k} n_{i, i} \\
N &= \sum_{i=0}^k \sum_{j=0}^k n_{i, j}
\end{align}
$$
2. Class wise accuracy
$$
\begin{align}
\text{Accuracy} &= \frac{1}{k} \sum_{i=0}^{k} \frac{n_{i, i}}{t_i} \\
t_i &= \sum_{j=0}^k n_{i, j}
\end{align}
$$

## 目的関数について

式(1) においてどの項が重要なのかを調べる実験をした．

###  質的評価

Fig 4. を見れば分かる通り，L1 のみでは理にかなった結果になるがボヤけてしまっている．またcGANのみ（式(1) において$\lambda=0$ ）ではよりハッキリした結果となるが，artifact がある．そして，L1 + cGAN ではartifact は減少している．

### 量的評価: FCN-score

FCN-score を用いて cityscape データの label -> photo 問題を量的に評価した(表１)．
GAN-based のロス関数は高いスコアを計測し，合成画像がより認識可能な構造を持つ．

- cGAN とGAN
  cGANとGAN を比較すると，GAN では入力と出力のミスマッチを罰せず，出力が realistic になるようにすることのみ考える．GAN ではGenerator が入力に関係なく，同一の出力を生成する様に崩壊した（mode collapse）ことが分かった．img2imgの場合，入力と出力のミスマッチを量的に測るロスが必要であると分かった．

- L1 の効果

  L1 loss は入力と出力とのミスマッチを罰するので，出力が入力に応じる様にすることが可能である．実際にL1 + GAN でも realistic なレンダリングという点で効果的であった． L1 + cGAN は同様に性能が良かった．

<img src="figures/table1.png" width=100% align="middle">





## Generator architecture





# 議論はある？

## Objective

- 実験ではDropout を適用することで出力$y$ に多様性を持たせようとした．しかし，Dropout を適用したのにも関わらず，それほど多様性を生むことができなかった．条件付き分布のフルエントロピーを得るようなcGAN を設計することは残された課題.
-



# 次に読むべき論文は？

- Wang et al. High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs : https://arxiv.org/abs/1711.11585

  pix2pixHD

- [Tensor flow の実行例](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/pix2pix/pix2pix_eager.ipynb)
