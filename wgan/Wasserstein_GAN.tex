\documentclass[uplatex, dvipdfmx]{jsarticle}

    \usepackage{amsmath, amssymb, amsthm}
    \usepackage{bm}
    \usepackage{ascmac}
    \usepackage[hiresbb]{graphicx}
    \usepackage{algpseudocode, algorithm}
    
    \theoremstyle{definition}
    \newtheorem{theorem}{定理}
    \newtheorem*{theorem*}{定理}
    \newtheorem{definition}[theorem]{定義}
    \newtheorem*{definition*}{定義}
    %%%%% argmax %%%%%
    \makeatletter
    \def\argmax{\mathop{\operator@font argmax}\limits}
    \makeatother

    %%%%% argmin %%%%%
    \makeatletter
    \def\argmin{\mathop{\operator@font argmin}\limits}
    
    % プリアンブル
    \title{Wassernstein GAN}
    \author{Martin Arjovsky et al.}
    \date{2017/1}
    \begin{document}
    \maketitle
    \abovedisplayskip=10.0pt% plus 4.0pt minus 6.0pt 
    \belowdisplayskip=10.0pt% plus 4.0pt minus 6.0pt % 
    % \setlength{\abovedisplayskip}{10pt} % 上部のマージン
    % \setlength{\belowdisplayskip}{5pt} % 下部のマージン      

    \section{Introduction}
    この論文が関係している問題は教師なし学習のそれである．主に問題は，確率分布を学習するということは
    どういうことなのか？である．古典的な回答は，確率密度を学ぶ事ということである．これはよく，パラメトリックな分布族$(P_{\theta})$を
    定義し，そしてデータにおいて尤度を最大にした$\theta$を見つけることによってなされる.

    もし，データ$\{x^{(i)}\}_{i=1}^{m}$ がある場合，以下の式を解くことでパラメータ$\theta$を求める．
    \begin{equation}
        \argmax_{\theta \in \mathbb{R}^{d}} \frac{1}{m} \sum_{i=1}^{m} \log P_{\theta}(x^{(i)})        
    \end{equation}
    もし，実データ分布$\mathbb{P}_{r}$が密度分布（連続）であるとすることができ，パラメータ化された確率密度$P_{\theta}$を$\mathbb{P}_{\theta}$
    とすると，漸近的に上式はKLダイバージェンス$KL(\mathbb{P}_{r} | \mathbb{P}_{r})$の最小化となる．
    
    存在しないであろう密度分布$\mathbb{P}_r$を推定するのではなく，私たちは固定された分布$p(z)$に従うランダムな変数$Z$を定義し，
    直接ある分布$\mathbb{P}_{\theta}$に従うサンプルを生成するパラメトリックな関数 $g_{\theta}: Z  \longmapsto \chi$（主に何らかのニューラルネットワー）
    に通すことができる．
    $\theta$を様々に変化させることによって，$\mathbb{P}_{\theta}$を変化させデータの分布$\mathbb{P}_{r}$と近づけることができる．これは次の２つにおいて役に立つ．
    まず最初に，densitiesと違い，この方法は低次元多様体に制限された分布を表すことができる．次に簡単にデータを生成できる能力は分布の数値的な値を知ることよりも
    役に立つ\footnote{例えば，超解像やセマンティックセグメンテーションの領域において入力が与えられて出力の条件付き分布を考える時に役に立つ}．
    一般には，任意の高次元密度分布と仮定し，サンプルを生成することは難しい\cite{Radford}．

    GANやVAEはこのアプローチのよく知られている例である．GANは目的関数の定義において柔軟であるが学習が難しいと知られている．

    この論文では私たちはモデルの分布$\mathbb{P}_{\theta}$とデータの分布$\mathbb{P}_{r}$がどの程度近いかを測る様々な方法や，
    同様に距離またはダイバージェンス$\rho (\mathbb{P}_{\theta}, \mathbb{P}_{r})$を定義する様々な方法に注意を向ける．
    
    この論文の主な貢献は以下の３つである．
    \begin{enumerate}
        \item 第二章では，Earth Mover (EM)距離がlearning distributionで使われる
        ポピュラーな距離，ダイバージェンスと比べて，どのように振る舞うのかの理論的な解析を行う．
        \item 第3章では，Wasserstein-GANと呼ばれ，妥当であり，効率の良いEM distance の近似を最小化するGANの
        形式を定義する．そして，対応する最適化問題が妥当，安定していることを理論的に証明する．
        \item 第4章では，経験的にWGANがGANの主な学習問題を解決することを示す．具体的には，WGANの学習はDiscriminatorとGenerator
        の学習において慎重にバランスを取ることを必要とせず，ニューラルネットワークの構造の慎重な設計も必要としない．
        GANに特有のmode dropping(collapse) 現象もまた劇的に減少させる．もっとも注目を惹きつけるWGANの実用的な利点の一つは
        Discriminatorを最適化することによって，連続的に(絶え間なく)EM距離を推定することができる能力である．
        これらの学習曲線をプロットすることはデバックやハイパーパラメータの探索に役に立つだけでなく，学習曲線は観測されたサンプルの質
        と強い相関がある．
    \end{enumerate}

    \begin{itembox}[l]{疑問点}
        \begin{enumerate}
            \item For this to make sense, we need the model density $P_{\theta}$
            to exist. This is not the case in the rather common situation where we are dealing with distributions supported 
            by low dimensional manifolds. It is then unlikely that the model manifold and the true distribution’s support have a non-negligible intersection 
            (see \cite{Martin}), and this means that the KL distance is not defined (or simply infinite). 
            
            なぜ，モデルの多様体と真の分布のサポート
            が無視できない交点を持つとKLは発散するのか．確かに2章のExample1は低次元多様体であり交点を持たない．その場合，KLは発散している．

            \item First of all, unlike densities, this approach can represent distributions confined to a low dimensional manifold.

            なぜ，低次元多様体に制限された分布を表現できるのか．また，低次元多様体の分布を表現しなければならないのは何故か．

            \item A sequence of distributions $(\mathbb{P}_{t})_{t \in \mathbb{N}}$ converges if and only if there is a distribution $\mathbb{P}_{\infty}$ 
            such that $\rho (\mathbb{P}_{t}, \mathbb{P}_{\infty})$ tends to zero, something that depends on how exactly the distance $\rho$ is defined. Informally, a distance 
            $\rho$ induces a weaker topology  when it makes it easier for a sequence of distribution to converge.
            
            A sequence of distributions とは? また，収束しやすいとき$\rho$は弱位相を含むとはどういうことなのか．

            \item The weaker this distance, the easier it is to define a continuous mapping from $\theta$-space to $\mathbb{P}_{\theta}$-space,
             since it’s easier for the distributions to converge.

            距離が弱いとはどういうことなのか．また，分布が収束しやすいため，距離が弱いと連続な$\theta$-space から $\mathbb{P}_{\theta}$-spaceへの
            写像が定義しやすいとはどういうことなのか．

            \item The main reason we care about the mapping $\theta  \longmapsto \mathbb{P}_{\theta}$ to be continuous is as follows. If 
            $\rho$ is our notion of distance between two distributions, we would like to have a loss function $\theta \longmapsto \rho (\mathbb{P}_{\theta}, \mathbb{P}_{r})$
            that is continuous, and this is equivalent to having the mapping $\theta \longmapsto \mathbb{P}_{\theta}$ 
            be continuous when using the distance between distributions $\rho$. 

            なぜ $\theta \longmapsto \mathbb{P}_{\theta}$ が連続ならば $\theta \longmapsto \rho (\mathbb{P}_{\theta}, \mathbb{P}_{r})$ も連続になるのか．
            % \item In order to optimize the parameter $\theta$, it is of course desirable to define our model distribution 
            % $\mathbb{P}_{\theta}$ in a manner that makes the mapping  $\theta  \longmapsto \mathbb{P}_{\theta}$continuous. 
            % Continuity means that when a sequence of parameters $\theta_{t}$converges to $\theta$, the distributions 
            % $\mathbb{P}_{\theta_t}$also converge to $\mathbb{P}_{\theta}$. However, it is essential to remember that the notion 
            % of the convergence of the distributions $\mathbb{P}_{\theta_t}$ depends on the way we compute the distance between distributions.

        \end{enumerate}
        
    \end{itembox}

    \section{Different Distance}

    \section{Wassernstein GAN}

    \section{Empirical Result}
    \section{Standard GANの問題点}

    \section{WGAN の改善点，利点}

    \section{実装}

    著者の実装について
    \begin{enumerate}
        \item $z$は opt.resize\_(opt.batchSize, nz, 1, 1).normal\_(0, 1)としてGenerater に与えている． 
        \item 前処理: 画像をスケーリング($64\times64$等に)しCenterCrop() したのち，Normalize((0.5, 0.5, 0.5), (0.5,0.5, 0.5))で正規化している．つまり
        各チャンネルごとに以下の数式で正規化している．
        \begin{equation}
            input[channel] = (input[channel] - mean[channel]) / std[channel]
        \end{equation}
        おそらく入力画像の画素値の取りうる範囲を$[-1, 1]$にしている．
        \item 初期値: Conv系(Convolution, Transpose)は平均0 , 分散0.02 の正規分布にて設定される． 
       また，Batch Normalizationの$\gamma$は平均1.0, 分散0.02の正規分布で初期化され，$\beta$は0で初期化される．

       \item GeneratorはConvTranspose2dとBatchNormalization, ReLUを繰り返す構造をしている．詳しくはnotebook参照．

       \item 論文では書いていないが，著者は
       \begin{quotation}
            The only addition to the code (that we forgot, and will add, on the paper) are the lines 163-166 of main.py.
            These lines act only on the first 25 generator iterations or very sporadically (once every 500 generator iterations).
            In such a case, they set the number of iterations on the critic to 100 instead of the default 5.
            This helps to start with the critic at optimum even in the first iterations. There shouldn't be a major difference in performance,
            but it can help, especially when visualizing learning curves (since otherwise you'd see the loss going up until the critic is properly trained).
            This is also why the first 25 iterations take significantly longer than the rest of the training as well.
        \end{quotation}
        と言っているのに注意

        \item また，コード$z$は平均0, 標準偏差1 の正規分布から取ってきている．
    \end{enumerate}
    \begin{itembox}[l]{疑問点}
        \begin{enumerate}
            \item Lipschitz 関数であるために重み$w \in \mathcal{W}$に対して$[-0.01, 0.01]$等にweight clipping を行うが，この場合
            Batch Normalizationの$\beta, \gamma$はどうすれば良いのか? こちらもまたweight clippingするべき?
            なにやら，Clitic（Discriminator）にweight clippingを施すと学習が崩壊することもあるらしい．

            \item $[−0.01,0.01] $を超えたものはclipされるため、初期値の分散が大きいと全て-0.01か0.01になる．よって重みの初期値をどう設定しているかが
            重要になりそう．
        \end{enumerate}
        

    \end{itembox}
    % 参考文
    \newpage
    \begin{thebibliography}{10}
        \bibitem{Radford}
        Radford M. Neal. Annealed importance sampling. 
        Statistics and Computing, 11(2):125–139, April 2001.

        \bibitem{Martin}
        Martin Arjovsky and L´eon Bottou.
        Towards principled methods for training generative adversarial networks. I \\
        https://arxiv.org/abs/1701.04862
    \end{thebibliography}
    
    \end{document}